{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import tqdm\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import sklearn.metrics as skmetrics\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from process_fe import create_feature_engineering_datasets\n",
    "from data import data_to_array_dict, get_data_date_split, get_data_date_id_split, get_feature_colnames\n",
    "from utils import stratification\n",
    "from plotting import paper_theme, ReliabilityDisplay, ShapDisplay, risk_feature_plot\n",
    "import metrics\n",
    "from shap_calculator import calc_shap_df\n",
    "\n",
    "from tqdm_style import tqdm_style\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the training set and predicting on the last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DAYS = 3\n",
    "THRESHOLDS = [0.3, 0.8]\n",
    "DATES_SPLIT = {\n",
    "    \"date_train_start\": \"2021-06-28\",\n",
    "    \"date_train_end\": \"2023-01-01\",\n",
    "    \"date_test_end\": \"2024-01-01\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nice_names = {\n",
    "    'awake_freq': 'Night time Awake Frequency', \n",
    "    'bathroom_daytime_freq': 'Daytime Bathroom Frequency', \n",
    "    'bathroom_daytime_freq_ma': 'Daytime Bathroom Frequency MA', \n",
    "    'bathroom_daytime_freq_ma_delta': 'Daytime Bathroom Frequency MA Delta', \n",
    "    'bathroom_freq': 'Bathroom Frequency',\n",
    "    'bathroom_nighttime_freq': 'Night time Bathroom Frequency', \n",
    "    'bathroom_nighttime_freq_ma': 'Night time Bathroom Frequency MA', \n",
    "    'bathroom_nighttime_freq_ma_delta': 'Night time Bathroom Frequency MA Delta', \n",
    "    'bathroom_relative_transition_time_delta_mean': 'Mean Relative Bathroom Transition Time Delta',\n",
    "    'bathroom_relative_transition_time_delta_std': 'STD Relative Bathroom Transition Time Delta',\n",
    "    'bedroom_freq': 'Bedroom Frequency',\n",
    "    'daily_entropy': 'Daily Entropy', \n",
    "    'hallway_freq': 'Hallway Frequency', \n",
    "    'heart_rate_mean': 'Mean Night Time Heart Rate',\n",
    "    'heart_rate_std': 'STD Night Time Heart Rate', \n",
    "    'kitchen_freq': 'Kitchen Frequency', \n",
    "    'lounge_freq': 'Lounge Frequency', \n",
    "    'previous_uti': 'Number of Previous UTIs',\n",
    "    'respiratory_rate_mean': 'Mean Night Time Respiratory Rate', \n",
    "    'respiratory_rate_std': 'STD Night Time Respiratory Rate',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_data = create_feature_engineering_datasets(reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train, data_test, _ = get_data_date_split(\n",
    "    fe_data, dates_split=DATES_SPLIT, n_days=N_DAYS, impute=True\n",
    ")\n",
    "\n",
    "\n",
    "X_train, y_train, ids_train, sample_weight = (\n",
    "    data_train['X'], data_train['y'], data_train[\"id\"], data_train['sample_weight']\n",
    ")\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to sets\n",
    "set_ids_train = set(ids_train)\n",
    "set_ids_test = set(ids_test)\n",
    "\n",
    "# Check for intersection\n",
    "common_ids = set_ids_train.intersection(set_ids_test)\n",
    "\n",
    "# Check if there are any common elements\n",
    "if common_ids:\n",
    "    print(f\"There are {len(common_ids)} common IDs between train and test datasets.\")\n",
    "else:\n",
    "    print(\"IDs in train and test datasets are unique to each other.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of IDs to remove\n",
    "ids_to_remove = ['AboZyUBeiQW3nVCcbXGpay', 'NZjrVTZQR1w9LPJMt26MbG', 'XVb8nztyc2LYPCAewZq11S', 'XdbAAiDw1vd3Bjbo9EVo1B']\n",
    "\n",
    "# Create a boolean index where False indicates IDs that need to be removed\n",
    "indices_to_keep = ~np.isin(data_test['id'], ids_to_remove)\n",
    "\n",
    "# Use this index to filter all related arrays in data_test\n",
    "data_test['X'] = data_test['X'][indices_to_keep]\n",
    "data_test['y'] = data_test['y'][indices_to_keep]\n",
    "data_test['id'] = data_test['id'][indices_to_keep]\n",
    "data_test['date'] = data_test['date'][indices_to_keep] if 'date' in data_test else None\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"id_train:\", ids_train.shape)\n",
    "print(\"id_test:\", ids_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here unique participants in the test dataset but not in the training dataset were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define flatten function\n",
    "def flatten(x):\n",
    "    return x.reshape(x.shape[0], -1)\n",
    "\n",
    "# Apply flattening\n",
    "X_train_flattened = flatten(X_train)\n",
    "X_test_flattened = flatten(X_test)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_test_scaled = scaler.transform(X_test_flattened)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "sample_weight_torch = torch.tensor(sample_weight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map['gender_encoded'] = gender_map['Gender PwD'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "\n",
    "patient_gender_dict = dict(zip(gender_map['patient_id'], gender_map['gender_encoded']))\n",
    "\n",
    "gender_train = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_train] \n",
    "\n",
    "gender_test = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_test] \n",
    "\n",
    "gender_train = np.array(gender_train).astype(int)\n",
    "gender_test = np.array(gender_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gender distribution in training data:\", np.unique(gender_train, return_counts=True))\n",
    "print(\"Gender distribution in testing data:\", np.unique(gender_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from patient IDs to numeric values\n",
    "unique_ids = np.unique(ids_train)\n",
    "id_to_numeric = {id_: i for i, id_ in enumerate(unique_ids)}\n",
    "\n",
    "# Apply the mapping to train and test IDs\n",
    "numeric_ids_train = np.array([id_to_numeric[id_] for id_ in ids_train])\n",
    "numeric_ids_test = np.array([id_to_numeric[id_] for id_ in ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, gender, patient_id, sample_weight=None):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.gender = torch.tensor(gender, dtype=torch.long)\n",
    "        self.patient_id = torch.tensor(patient_id, dtype=torch.long)  # Ensure patient_id is Long\n",
    "        if sample_weight is not None:\n",
    "            self.sample_weight = torch.tensor(sample_weight, dtype=torch.float32)\n",
    "        else:\n",
    "            self.sample_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sample_weight is not None:\n",
    "            return self.features[index], self.labels[index], self.gender[index], self.patient_id[index], self.sample_weight[index]\n",
    "        return self.features[index], self.labels[index], self.gender[index], self.patient_id[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = CustomDataset(X_train_scaled, y_train, gender_train, numeric_ids_train, sample_weight)\n",
    "test_dataset = CustomDataset(X_test_scaled, y_test, gender_test, numeric_ids_test, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, num_patient_ids, embedding_dim=8, dropout_rate=0.5):\n",
    "        super(MLPWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_patient_ids, embedding_dim=embedding_dim)\n",
    "        self.layer1 = nn.Linear(input_size + embedding_dim, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, patient_id):\n",
    "        patient_id_embedded = self.embedding(patient_id)\n",
    "        x = torch.cat((x, patient_id_embedded), dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for features, labels, gender, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, patient_id)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, gender, patient_id, sample_weight in val_loader:\n",
    "                outputs = model(features, patient_id)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_and_tune(train_dataset, num_unique_ids, param_grid, num_folds=10, num_epochs=10):\n",
    "    best_hyperparams = None\n",
    "    best_val_loss = float('inf')\n",
    "    best_metrics = None\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_folds)\n",
    "    \n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"Testing hyperparameters: {params}\")\n",
    "        \n",
    "        fold_metrics = {\n",
    "            'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "            'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "            'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "        }\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "            print(f'Fold {fold+1}/{num_folds} with params: {params}')\n",
    "            \n",
    "            train_subset = Subset(train_dataset, train_index)\n",
    "            val_subset = Subset(train_dataset, val_index)\n",
    "            train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "            print(f\"Train loader length: {len(train_loader)}, Validation loader length: {len(val_loader)}\")\n",
    "            \n",
    "            model = MLPWithEmbedding(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, num_patient_ids=num_unique_ids, embedding_dim=8, dropout_rate=params['dropout_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            \n",
    "            val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "            \n",
    "            fold_metrics['accuracy'].append(accuracy)\n",
    "            fold_metrics['precision'].append(precision)\n",
    "            fold_metrics['sensitivity'].append(sensitivity)\n",
    "            fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "            fold_metrics['male_precision'].append(male_precision)\n",
    "            fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "            fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "            fold_metrics['female_precision'].append(female_precision)\n",
    "            fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "        \n",
    "        avg_val_loss = np.mean([val_loss])\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_hyperparams = params\n",
    "            best_metrics = fold_metrics\n",
    "\n",
    "        # Print the metrics for each combination of hyperparameters at the end of 10 folds\n",
    "        print(f'Hyperparameters: {params}')\n",
    "        for metric in fold_metrics:\n",
    "            print(f'{metric}: Mean = {np.mean(fold_metrics[metric]):.4f}, Std = {np.std(fold_metrics[metric]):.4f}')\n",
    "\n",
    "    # Print the best hyperparameters and corresponding metrics\n",
    "    print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "    for metric in best_metrics:\n",
    "        print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')\n",
    "\n",
    "    return best_hyperparams\n",
    "\n",
    "param_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "num_unique_ids = len(np.unique(ids_train))\n",
    "\n",
    "# Perform cross-validation and find the best hyperparameters\n",
    "best_hyperparams = cross_validate_and_tune(train_dataset, num_unique_ids, param_grid, num_folds=10, num_epochs=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bootstrap sampling function\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    unique_patient_ids = np.unique(patient_ids)\n",
    "    for pid in unique_patient_ids:\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(embedding_mlp_model, dataloader):\n",
    "    embedding_mlp_model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels, gender, patient_id, sample_weight in dataloader:\n",
    "            outputs = embedding_mlp_model(features, patient_id)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    males = [i for i, g in enumerate(all_genders) if g == 0]\n",
    "    females = [i for i, g in enumerate(all_genders) if g == 1]\n",
    "\n",
    "    male_labels = [all_labels[i] for i in males]\n",
    "    male_preds = [all_preds[i] for i in males]\n",
    "    female_labels = [all_labels[i] for i in females]\n",
    "    female_preds = [all_preds[i] for i in females]\n",
    "\n",
    "    male_accuracy = accuracy_score(male_labels, male_preds)\n",
    "    male_precision = precision_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "    male_recall = recall_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "    male_f1 = f1_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(female_labels, female_preds)\n",
    "    female_precision = precision_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "    female_recall = recall_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "    female_f1 = f1_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'overall': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        },\n",
    "        'male': {\n",
    "            'accuracy': male_accuracy,\n",
    "            'precision': male_precision,\n",
    "            'recall': male_recall,\n",
    "            'f1': male_f1\n",
    "        },\n",
    "        'female': {\n",
    "            'accuracy': female_accuracy,\n",
    "            'precision': female_precision,\n",
    "            'recall': female_recall,\n",
    "            'f1': female_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Perform cross-validation and find the best hyperparameters\n",
    "param_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "num_unique_ids = len(np.unique(ids_train))\n",
    "best_hyperparams = cross_validate_and_tune(train_dataset, num_unique_ids, param_grid, num_folds=10, num_epochs=10)\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each patient\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    model = MLPWithEmbedding(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, num_patient_ids=num_unique_ids, embedding_dim=8, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset, batch_size=128, shuffle=False), criterion, optimizer, num_epochs=10)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results['overall']['accuracy'])\n",
    "    bootstrap_results['precision'].append(results['overall']['precision'])\n",
    "    bootstrap_results['sensitivity'].append(results['overall']['recall'])\n",
    "    bootstrap_results['male_accuracy'].append(results['male']['accuracy'])\n",
    "    bootstrap_results['male_precision'].append(results['male']['precision'])\n",
    "    bootstrap_results['male_sensitivity'].append(results['male']['recall'])\n",
    "    bootstrap_results['female_accuracy'].append(results['female']['accuracy'])\n",
    "    bootstrap_results['female_precision'].append(results['female']['precision'])\n",
    "    bootstrap_results['female_sensitivity'].append(results['female']['recall'])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uti-minder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
