{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import tqdm\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import sklearn.metrics as skmetrics\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from process_fe import create_feature_engineering_datasets\n",
    "from data import data_to_array_dict, get_data_date_split, get_data_date_id_split, get_feature_colnames\n",
    "from utils import stratification\n",
    "from plotting import paper_theme, ReliabilityDisplay, ShapDisplay, risk_feature_plot\n",
    "import metrics\n",
    "from shap_calculator import calc_shap_df\n",
    "\n",
    "from tqdm_style import tqdm_style\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the training set and predicting on the last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DAYS = 3\n",
    "THRESHOLDS = [0.3, 0.8]\n",
    "DATES_SPLIT = {\n",
    "    \"date_train_start\": \"2021-06-28\",\n",
    "    \"date_train_end\": \"2023-01-01\",\n",
    "    \"date_test_end\": \"2024-01-01\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nice_names = {\n",
    "    'awake_freq': 'Night time Awake Frequency', \n",
    "    'bathroom_daytime_freq': 'Daytime Bathroom Frequency', \n",
    "    'bathroom_daytime_freq_ma': 'Daytime Bathroom Frequency MA', \n",
    "    'bathroom_daytime_freq_ma_delta': 'Daytime Bathroom Frequency MA Delta', \n",
    "    'bathroom_freq': 'Bathroom Frequency',\n",
    "    'bathroom_nighttime_freq': 'Night time Bathroom Frequency', \n",
    "    'bathroom_nighttime_freq_ma': 'Night time Bathroom Frequency MA', \n",
    "    'bathroom_nighttime_freq_ma_delta': 'Night time Bathroom Frequency MA Delta', \n",
    "    'bathroom_relative_transition_time_delta_mean': 'Mean Relative Bathroom Transition Time Delta',\n",
    "    'bathroom_relative_transition_time_delta_std': 'STD Relative Bathroom Transition Time Delta',\n",
    "    'bedroom_freq': 'Bedroom Frequency',\n",
    "    'daily_entropy': 'Daily Entropy', \n",
    "    'hallway_freq': 'Hallway Frequency', \n",
    "    'heart_rate_mean': 'Mean Night Time Heart Rate',\n",
    "    'heart_rate_std': 'STD Night Time Heart Rate', \n",
    "    'kitchen_freq': 'Kitchen Frequency', \n",
    "    'lounge_freq': 'Lounge Frequency', \n",
    "    'previous_uti': 'Number of Previous UTIs',\n",
    "    'respiratory_rate_mean': 'Mean Night Time Respiratory Rate', \n",
    "    'respiratory_rate_std': 'STD Night Time Respiratory Rate',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_data = create_feature_engineering_datasets(reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train, data_test, _ = get_data_date_split(\n",
    "    fe_data, dates_split=DATES_SPLIT, n_days=N_DAYS, impute=True\n",
    ")\n",
    "\n",
    "\n",
    "X_train, y_train, ids_train, sample_weight = (\n",
    "    data_train['X'], data_train['y'], data_train[\"id\"], data_train['sample_weight']\n",
    ")\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to sets\n",
    "set_ids_train = set(ids_train)\n",
    "set_ids_test = set(ids_test)\n",
    "\n",
    "# Check for intersection\n",
    "common_ids = set_ids_train.intersection(set_ids_test)\n",
    "\n",
    "# Check if there are any common elements\n",
    "if common_ids:\n",
    "    print(f\"There are {len(common_ids)} common IDs between train and test datasets.\")\n",
    "else:\n",
    "    print(\"IDs in train and test datasets are unique to each other.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of IDs to remove\n",
    "ids_to_remove = ['AboZyUBeiQW3nVCcbXGpay', 'NZjrVTZQR1w9LPJMt26MbG', 'XVb8nztyc2LYPCAewZq11S', 'XdbAAiDw1vd3Bjbo9EVo1B']\n",
    "\n",
    "# Create a boolean index where False indicates IDs that need to be removed\n",
    "indices_to_keep = ~np.isin(data_test['id'], ids_to_remove)\n",
    "\n",
    "# Use this index to filter all related arrays in data_test\n",
    "data_test['X'] = data_test['X'][indices_to_keep]\n",
    "data_test['y'] = data_test['y'][indices_to_keep]\n",
    "data_test['id'] = data_test['id'][indices_to_keep]\n",
    "data_test['date'] = data_test['date'][indices_to_keep] if 'date' in data_test else None\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"id_train:\", ids_train.shape)\n",
    "print(\"id_test:\", ids_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a pandas Series\n",
    "ids_train_series = pd.Series(ids_train)\n",
    "\n",
    "# Now you can use the nunique() method\n",
    "unique_count = ids_train_series.nunique()\n",
    "\n",
    "print(\"Number of unique elements in ids_train:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train_series = pd.Series(ids_test)\n",
    "\n",
    "# Now you can use the nunique() method\n",
    "unique_count = ids_train_series.nunique()\n",
    "\n",
    "print(\"Number of unique elements in ids_test:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define flatten function\n",
    "def flatten(x):\n",
    "    return x.reshape(x.shape[0], -1)\n",
    "\n",
    "# Apply flattening\n",
    "X_train_flattened = flatten(X_train)\n",
    "X_test_flattened = flatten(X_test)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_test_scaled = scaler.transform(X_test_flattened)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "sample_weight_torch = torch.tensor(sample_weight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map['gender_encoded'] = gender_map['Gender PwD'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "\n",
    "patient_gender_dict = dict(zip(gender_map['patient_id'], gender_map['gender_encoded']))\n",
    "\n",
    "gender_train = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_train] \n",
    "\n",
    "gender_test = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_test] \n",
    "\n",
    "gender_train = np.array(gender_train).astype(int)\n",
    "gender_test = np.array(gender_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for 'Does PwD live alone?'\n",
    "carer_map = {'No': 0, 'Yes': 1, 'Care home': 2}\n",
    "\n",
    "# Map 'Does PwD live alone?' to numeric values\n",
    "gender_map['carer_encoded'] = gender_map['Does PwD live alone?'].map(carer_map)\n",
    "\n",
    "# Create carer_dict mapping patient_id to carer_encoded\n",
    "carer_dict = dict(zip(gender_map['patient_id'], gender_map['carer_encoded']))\n",
    "\n",
    "# Generate carer_train and carer_test\n",
    "carer_train = [carer_dict.get(patient_id, 0) for patient_id in ids_train]\n",
    "carer_test = [carer_dict.get(patient_id, 0) for patient_id in ids_test]\n",
    "\n",
    "carer_train = np.array(carer_train).astype(int)\n",
    "carer_test = np.array(carer_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gender distribution in training data:\", np.unique(gender_train, return_counts=True))\n",
    "print(\"Gender distribution in testing data:\", np.unique(gender_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, gender, carer_status, patient_id, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Constructor for the dataset.\n",
    "        :param features: The input features (numpy array).\n",
    "        :param labels: The labels corresponding to the features (numpy array).\n",
    "        :param loss_labels: The loss labels corresponding to the features (numpy array).\n",
    "        :param gender: The gender information (numpy array).\n",
    "        :param carer_status: The carer status information (numpy array).\n",
    "        :param patient_id: The patient ID information (numpy array).\n",
    "        :param sample_weight: Optional sample weights (numpy array).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        self.gender = torch.tensor(gender, dtype=torch.long)  # Assuming gender is categorical\n",
    "        self.carer_status = torch.tensor(carer_status, dtype=torch.long)  # Assuming carer status is categorical\n",
    "        self.patient_id = patient_id\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            self.sample_weight = torch.tensor(sample_weight, dtype=torch.float32)\n",
    "        else:\n",
    "            self.sample_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sample_weight is not None:\n",
    "            return (self.features[index], self.labels[index], \n",
    "                    self.gender[index], self.carer_status[index], self.sample_weight[index], \n",
    "                    self.patient_id[index])\n",
    "        return (self.features[index], self.labels[index], \n",
    "                self.gender[index], self.carer_status[index], self.patient_id[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = CustomDataset(X_train_scaled, y_train, gender_train, carer_train, ids_train, sample_weight)\n",
    "test_dataset = CustomDataset(X_test_scaled, y_test, gender_test, carer_test, ids_test, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameter_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, labels, gender, carer_status, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, gender, carer_status, patient_i, sample_weight in val_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        # Calculate metrics for males and females separately\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with hyperparameter tuning\n",
    "best_hyperparams = None\n",
    "best_val_loss = float('inf')\n",
    "best_metrics = None\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for params in ParameterGrid(hyperparameter_grid):\n",
    "    fold_metrics = {\n",
    "        'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "        'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "        'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "    }\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "        print(f'Fold {fold+1} with params: {params}')\n",
    "        train_subset = Subset(train_dataset, train_index)\n",
    "        val_subset = Subset(train_dataset, val_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "        model = MLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=params['dropout_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "        \n",
    "        fold_metrics['accuracy'].append(accuracy)\n",
    "        fold_metrics['precision'].append(precision)\n",
    "        fold_metrics['sensitivity'].append(sensitivity)\n",
    "        fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "        fold_metrics['male_precision'].append(male_precision)\n",
    "        fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "        fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "        fold_metrics['female_precision'].append(female_precision)\n",
    "        fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "    \n",
    "    avg_val_loss = np.mean([val_loss])\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_hyperparams = params\n",
    "        best_metrics = fold_metrics\n",
    "\n",
    "# Print the best hyperparameters and corresponding metrics\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "for metric in best_metrics:\n",
    "    print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap sampling\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    for pid in np.unique(patient_ids):\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels, gender, carer_status, patient_id, sample_weight in data_loader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "\n",
    "    total_loss /= len(data_loader)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_genders = np.array(all_genders)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    sensitivity = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    male_indices = all_genders == 0\n",
    "    female_indices = all_genders == 1\n",
    "\n",
    "    male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "    male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='macro', zero_division=0)\n",
    "    male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='macro', zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "    female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='macro', zero_division=0)\n",
    "    female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each patient\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    model = MLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset, batch_size=128, shuffle=False), criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader, criterion)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results[1])\n",
    "    bootstrap_results['precision'].append(results[2])\n",
    "    bootstrap_results['sensitivity'].append(results[3])\n",
    "    bootstrap_results['male_accuracy'].append(results[4])\n",
    "    bootstrap_results['male_precision'].append(results[5])\n",
    "    bootstrap_results['male_sensitivity'].append(results[6])\n",
    "    bootstrap_results['female_accuracy'].append(results[7])\n",
    "    bootstrap_results['female_precision'].append(results[8])\n",
    "    bootstrap_results['female_sensitivity'].append(results[9])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model with concatenation\n",
    "class concat_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate):\n",
    "        super(concat_MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1 + 1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.layer3 = nn.Linear(hidden_size2 + 1, output_size)\n",
    "\n",
    "    def forward(self, x, gender):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.cat((x, gender.unsqueeze(1)), dim=1)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.cat((x, gender.unsqueeze(1)), dim=1)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, labels, gender, carer_status, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, gender)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, gender, carer_status, patient_id, sample_weight in val_loader:\n",
    "                outputs = model(features, gender)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        # Calculate metrics for males and females separately\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "hyperparameter_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "\n",
    "# Cross-validation\n",
    "best_hyperparams = None\n",
    "best_val_loss = float('inf')\n",
    "best_metrics = None\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for params in ParameterGrid(hyperparameter_grid):\n",
    "    fold_metrics = {\n",
    "        'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "        'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "        'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "    }\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "        print(f'Fold {fold+1} with params: {params}')\n",
    "        train_subset = Subset(train_dataset, train_index)\n",
    "        val_subset = Subset(train_dataset, val_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "        model = concat_MLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=params['dropout_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "        \n",
    "        fold_metrics['accuracy'].append(accuracy)\n",
    "        fold_metrics['precision'].append(precision)\n",
    "        fold_metrics['sensitivity'].append(sensitivity)\n",
    "        fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "        fold_metrics['male_precision'].append(male_precision)\n",
    "        fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "        fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "        fold_metrics['female_precision'].append(female_precision)\n",
    "        fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "    \n",
    "    avg_val_loss = np.mean([val_loss])\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_hyperparams = params\n",
    "        best_metrics = fold_metrics\n",
    "\n",
    "# Print the best hyperparameters and corresponding metrics\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "for metric in best_metrics:\n",
    "    print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap sampling\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    for pid in np.unique(patient_ids):\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, gender, carer_status, sample_weight, patient_id in dataloader:\n",
    "            outputs = model(inputs, gender)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "\n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    # Gender-specific metrics\n",
    "    males = [i for i, g in enumerate(all_genders) if g == 0]\n",
    "    females = [i for i, g in enumerate(all_genders) if g == 1]\n",
    "\n",
    "    male_labels = [all_labels[i] for i in males]\n",
    "    male_preds = [all_preds[i] for i in males]\n",
    "    female_labels = [all_labels[i] for i in females]\n",
    "    female_preds = [all_preds[i] for i in females]\n",
    "\n",
    "    male_accuracy = accuracy_score(male_labels, male_preds)\n",
    "    male_precision = precision_score(male_labels, male_preds, zero_division=0)\n",
    "    male_recall = recall_score(male_labels, male_preds, zero_division=0)  # Sensitivity\n",
    "    male_f1 = f1_score(male_labels, male_preds, zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(female_labels, female_preds)\n",
    "    female_precision = precision_score(female_labels, female_preds, zero_division=0)\n",
    "    female_recall = recall_score(female_labels, female_preds, zero_division=0)  # Sensitivity\n",
    "    female_f1 = f1_score(female_labels, female_preds, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'overall': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "        },\n",
    "        'male': {\n",
    "            'accuracy': male_accuracy,\n",
    "            'precision': male_precision,\n",
    "            'recall': male_recall,\n",
    "            'f1': male_f1\n",
    "        },\n",
    "        'female': {\n",
    "            'accuracy': female_accuracy,\n",
    "            'precision': female_precision,\n",
    "            'recall': female_recall,\n",
    "            'f1': female_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each patient\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    model = concat_MLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset, batch_size=128, shuffle=False), criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results['overall']['accuracy'])\n",
    "    bootstrap_results['precision'].append(results['overall']['precision'])\n",
    "    bootstrap_results['sensitivity'].append(results['overall']['recall'])\n",
    "    bootstrap_results['male_accuracy'].append(results['male']['accuracy'])\n",
    "    bootstrap_results['male_precision'].append(results['male']['precision'])\n",
    "    bootstrap_results['male_sensitivity'].append(results['male']['recall'])\n",
    "    bootstrap_results['female_accuracy'].append(results['female']['accuracy'])\n",
    "    bootstrap_results['female_precision'].append(results['female']['precision'])\n",
    "    bootstrap_results['female_sensitivity'].append(results['female']['recall'])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model with embedding and dropout\n",
    "class MLPWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, embedding_dim=4, dropout_rate=0):\n",
    "        super(MLPWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=2, embedding_dim=embedding_dim)\n",
    "        self.layer1 = nn.Linear(input_size + embedding_dim, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, gender):\n",
    "        gender_embedded = self.embedding(gender)\n",
    "        x = torch.cat((x, gender_embedded), dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for features, labels, gender, carer_status, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, gender)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, gender, carer_status, patient_id, sample_weight in val_loader:\n",
    "                outputs = model(features, gender)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        # Calculate metrics for males and females separately\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "hyperparameter_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "\n",
    "# Cross-validation with hyperparameter tuning\n",
    "best_hyperparams = None\n",
    "best_val_loss = float('inf')\n",
    "best_metrics = None\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for params in ParameterGrid(hyperparameter_grid):\n",
    "    fold_metrics = {\n",
    "        'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "        'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "        'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "    }\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "        print(f'Fold {fold+1} with params: {params}')\n",
    "        train_subset = Subset(train_dataset, train_index)\n",
    "        val_subset = Subset(train_dataset, val_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "        model = MLPWithEmbedding(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, embedding_dim=4, dropout_rate=params['dropout_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "        \n",
    "        fold_metrics['accuracy'].append(accuracy)\n",
    "        fold_metrics['precision'].append(precision)\n",
    "        fold_metrics['sensitivity'].append(sensitivity)\n",
    "        fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "        fold_metrics['male_precision'].append(male_precision)\n",
    "        fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "        fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "        fold_metrics['female_precision'].append(female_precision)\n",
    "        fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "    \n",
    "    avg_val_loss = np.mean([val_loss])\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_hyperparams = params\n",
    "        best_metrics = fold_metrics\n",
    "\n",
    "# Print the best hyperparameters and corresponding metrics\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "for metric in best_metrics:\n",
    "    print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap sampling\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    for pid in np.unique(patient_ids):\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels, gender, carer_status, patient_id, sample_weight in dataloader:\n",
    "            outputs = model(features, gender)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "\n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    # Gender-specific metrics\n",
    "    males = [i for i, g in enumerate(all_genders) if g == 0]\n",
    "    females = [i for i, g in enumerate(all_genders) if g == 1]\n",
    "\n",
    "    male_labels = [all_labels[i] for i in males]\n",
    "    male_preds = [all_preds[i] for i in males]\n",
    "    female_labels = [all_labels[i] for i in females]\n",
    "    female_preds = [all_preds[i] for i in females]\n",
    "\n",
    "    male_accuracy = accuracy_score(male_labels, male_preds)\n",
    "    male_precision = precision_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "    male_recall = recall_score(male_labels, male_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    male_f1 = f1_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(female_labels, female_preds)\n",
    "    female_precision = precision_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "    female_recall = recall_score(female_labels, female_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    female_f1 = f1_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'overall': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        },\n",
    "        'male': {\n",
    "            'accuracy': male_accuracy,\n",
    "            'precision': male_precision,\n",
    "            'recall': male_recall,\n",
    "            'f1': male_f1\n",
    "        },\n",
    "        'female': {\n",
    "            'accuracy': female_accuracy,\n",
    "            'precision': female_precision,\n",
    "            'recall': female_recall,\n",
    "            'f1': female_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each participant\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    model = MLPWithEmbedding(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, embedding_dim=4, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset, batch_size=128, shuffle=False), criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results['overall']['accuracy'])\n",
    "    bootstrap_results['precision'].append(results['overall']['precision'])\n",
    "    bootstrap_results['sensitivity'].append(results['overall']['recall'])\n",
    "    bootstrap_results['male_accuracy'].append(results['male']['accuracy'])\n",
    "    bootstrap_results['male_precision'].append(results['male']['precision'])\n",
    "    bootstrap_results['male_sensitivity'].append(results['male']['recall'])\n",
    "    bootstrap_results['female_accuracy'].append(results['female']['accuracy'])\n",
    "    bootstrap_results['female_precision'].append(results['female']['precision'])\n",
    "    bootstrap_results['female_sensitivity'].append(results['female']['recall'])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uti-minder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
