{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import tqdm\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import sklearn.metrics as skmetrics\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from process_fe import create_feature_engineering_datasets\n",
    "from data import data_to_array_dict, get_data_date_split, get_data_date_id_split, get_feature_colnames\n",
    "from utils import stratification\n",
    "from plotting import paper_theme, ReliabilityDisplay, ShapDisplay, risk_feature_plot\n",
    "import metrics\n",
    "from shap_calculator import calc_shap_df\n",
    "\n",
    "from tqdm_style import tqdm_style\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the training set and predicting on the last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DAYS = 3\n",
    "THRESHOLDS = [0.3, 0.8]\n",
    "DATES_SPLIT = {\n",
    "    \"date_train_start\": \"2021-06-28\",\n",
    "    \"date_train_end\": \"2023-01-01\",\n",
    "    \"date_test_end\": \"2024-01-01\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nice_names = {\n",
    "    'awake_freq': 'Night time Awake Frequency', \n",
    "    'bathroom_daytime_freq': 'Daytime Bathroom Frequency', \n",
    "    'bathroom_daytime_freq_ma': 'Daytime Bathroom Frequency MA', \n",
    "    'bathroom_daytime_freq_ma_delta': 'Daytime Bathroom Frequency MA Delta', \n",
    "    'bathroom_freq': 'Bathroom Frequency',\n",
    "    'bathroom_nighttime_freq': 'Night time Bathroom Frequency', \n",
    "    'bathroom_nighttime_freq_ma': 'Night time Bathroom Frequency MA', \n",
    "    'bathroom_nighttime_freq_ma_delta': 'Night time Bathroom Frequency MA Delta', \n",
    "    'bathroom_relative_transition_time_delta_mean': 'Mean Relative Bathroom Transition Time Delta',\n",
    "    'bathroom_relative_transition_time_delta_std': 'STD Relative Bathroom Transition Time Delta',\n",
    "    'bedroom_freq': 'Bedroom Frequency',\n",
    "    'daily_entropy': 'Daily Entropy', \n",
    "    'hallway_freq': 'Hallway Frequency', \n",
    "    'heart_rate_mean': 'Mean Night Time Heart Rate',\n",
    "    'heart_rate_std': 'STD Night Time Heart Rate', \n",
    "    'kitchen_freq': 'Kitchen Frequency', \n",
    "    'lounge_freq': 'Lounge Frequency', \n",
    "    'previous_uti': 'Number of Previous UTIs',\n",
    "    'respiratory_rate_mean': 'Mean Night Time Respiratory Rate', \n",
    "    'respiratory_rate_std': 'STD Night Time Respiratory Rate',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_data = create_feature_engineering_datasets(reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train, data_test, _ = get_data_date_split(\n",
    "    fe_data, dates_split=DATES_SPLIT, n_days=N_DAYS, impute=True\n",
    ")\n",
    "\n",
    "\n",
    "X_train, y_train, ids_train, sample_weight = (\n",
    "    data_train['X'], data_train['y'], data_train[\"id\"], data_train['sample_weight']\n",
    ")\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to sets\n",
    "set_ids_train = set(ids_train)\n",
    "set_ids_test = set(ids_test)\n",
    "\n",
    "# Check for intersection\n",
    "common_ids = set_ids_train.intersection(set_ids_test)\n",
    "\n",
    "# Check if there are any common elements\n",
    "if common_ids:\n",
    "    print(f\"There are {len(common_ids)} common IDs between train and test datasets.\")\n",
    "else:\n",
    "    print(\"IDs in train and test datasets are unique to each other.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of IDs to remove\n",
    "ids_to_remove = ['AboZyUBeiQW3nVCcbXGpay', 'NZjrVTZQR1w9LPJMt26MbG', 'XVb8nztyc2LYPCAewZq11S', 'XdbAAiDw1vd3Bjbo9EVo1B']\n",
    "\n",
    "# Create a boolean index where False indicates IDs that need to be removed\n",
    "indices_to_keep = ~np.isin(data_test['id'], ids_to_remove)\n",
    "\n",
    "# Use this index to filter all related arrays in data_test\n",
    "data_test['X'] = data_test['X'][indices_to_keep]\n",
    "data_test['y'] = data_test['y'][indices_to_keep]\n",
    "data_test['id'] = data_test['id'][indices_to_keep]\n",
    "data_test['date'] = data_test['date'][indices_to_keep] if 'date' in data_test else None\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"id_train:\", ids_train.shape)\n",
    "print(\"id_test:\", ids_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a pandas Series\n",
    "ids_train_series = pd.Series(ids_train)\n",
    "\n",
    "# Now you can use the nunique() method\n",
    "unique_count = ids_train_series.nunique()\n",
    "\n",
    "print(\"Number of unique elements in ids_train:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train_series = pd.Series(ids_test)\n",
    "\n",
    "# Now you can use the nunique() method\n",
    "unique_count = ids_train_series.nunique()\n",
    "\n",
    "print(\"Number of unique elements in ids_test:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define flatten function\n",
    "def flatten(x):\n",
    "    return x.reshape(x.shape[0], -1)\n",
    "\n",
    "# Apply flattening\n",
    "X_train_flattened = flatten(X_train)\n",
    "X_test_flattened = flatten(X_test)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_test_scaled = scaler.transform(X_test_flattened)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "sample_weight_torch = torch.tensor(sample_weight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map['gender_encoded'] = gender_map['Gender PwD'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "\n",
    "patient_gender_dict = dict(zip(gender_map['patient_id'], gender_map['gender_encoded']))\n",
    "\n",
    "gender_train = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_train] \n",
    "\n",
    "gender_test = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_test] \n",
    "\n",
    "gender_train = np.array(gender_train).astype(int)\n",
    "gender_test = np.array(gender_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gender distribution in training data:\", np.unique(gender_train, return_counts=True))\n",
    "print(\"Gender distribution in testing data:\", np.unique(gender_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, gender, patient_id, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Constructor for the dataset.\n",
    "        :param features: The input features (numpy array).\n",
    "        :param labels: The labels corresponding to the features (numpy array).\n",
    "        :param gender: The gender information (numpy array).\n",
    "        :param sample_weight: Optional sample weights (numpy array).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        self.gender = torch.tensor(gender, dtype=torch.long)  # Assuming gender is categorical\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            self.sample_weight = torch.tensor(sample_weight, dtype=torch.float32)\n",
    "        else:\n",
    "            self.sample_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sample_weight is not None:\n",
    "            return self.features[index], self.labels[index],self.loss_labels[index], self.gender[index], self.sample_weight[index], self.patient_id[index]\n",
    "        return self.features[index], self.labels[index], self.loss_labels[index], self.patient_id[index]\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = CustomDataset(X_train_scaled, y_train, gender_train, ids_train, sample_weight)\n",
    "test_dataset = CustomDataset(X_test_scaled, y_test, gender_test, ids_test, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define the range for the number of clusters\n",
    "cluster_range = range(2, 11)\n",
    "\n",
    "# Dictionary to store silhouette scores\n",
    "silhouette_scores = {}\n",
    "\n",
    "# Perform KMeans clustering for each number of clusters\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_train_scaled)\n",
    "    silhouette_avg = silhouette_score(X_train_scaled, cluster_labels)\n",
    "    silhouette_scores[n_clusters] = silhouette_avg\n",
    "    print(f\"For n_clusters = {n_clusters}, the silhouette score is {silhouette_avg}\")\n",
    "\n",
    "# Convert silhouette scores to a DataFrame for easier visualization\n",
    "silhouette_df = pd.DataFrame(list(silhouette_scores.items()), columns=['Number of Clusters', 'Silhouette Score'])\n",
    "silhouette_df = silhouette_df.sort_values(by='Silhouette Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(silhouette_df)\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(silhouette_df['Number of Clusters'], silhouette_df['Silhouette Score'], color='skyblue')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the KMeans model with the optimal number of clusters on the train data\n",
    "optimal_clusters = 2\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "cluster_train = kmeans.fit_predict(X_train_scaled)\n",
    "\n",
    "# Predict clusters for the test data using the same KMeans model\n",
    "cluster_test = kmeans.predict(X_test_scaled)\n",
    "\n",
    "# Print the number of points in each cluster\n",
    "print(f\"Number of points in cluster 0 (train): {np.sum(cluster_train == 0)}\")\n",
    "print(f\"Number of points in cluster 1 (train): {np.sum(cluster_train == 1)}\")\n",
    "print(f\"Number of points in cluster 0 (test): {np.sum(cluster_test == 0)}\")\n",
    "print(f\"Number of points in cluster 1 (test): {np.sum(cluster_test == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset0(Dataset):\n",
    "    def __init__(self, features, labels, gender, patient_id, cluster, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Constructor for the dataset.\n",
    "        :param features: The input features (numpy array).\n",
    "        :param labels: The labels corresponding to the features (numpy array).\n",
    "        :param gender: The gender information (numpy array).\n",
    "        :param patient_id: The patient ID information (list or numpy array).\n",
    "        :param cluster: The cluster information (numpy array).\n",
    "        :param sample_weight: Optional sample weights (numpy array).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        self.gender = torch.tensor(gender, dtype=torch.long)  # Assuming gender is categorical\n",
    "        self.cluster = torch.tensor(cluster, dtype=torch.long)  # Assuming cluster is categorical\n",
    "        self.patient_id = patient_id\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            print(f\"Sample weight type: {type(sample_weight)}, shape: {np.shape(sample_weight)}\")\n",
    "            self.sample_weight = torch.tensor(sample_weight, dtype=torch.float32)\n",
    "        else:\n",
    "            self.sample_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sample_weight is not None:\n",
    "            return (self.features[index], self.labels[index], \n",
    "                    self.gender[index], self.cluster[index], self.sample_weight[index], \n",
    "                    self.patient_id[index])\n",
    "        return (self.features[index], self.labels[index], \n",
    "                self.gender[index], self.cluster[index], self.patient_id[index])\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset_0 = CustomDataset0(X_train_scaled, y_train, gender_train, ids_train, cluster_train, sample_weight)\n",
    "test_dataset_0 = CustomDataset0(X_test_scaled, y_test, gender_test, ids_test, cluster_test, sample_weight)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 128  # You can adjust this based on your system's capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of features in train dataset: {train_dataset_0.features.shape}\")\n",
    "print(f\"Shape of labels in train dataset: {train_dataset_0.labels.shape}\")\n",
    "print(f\"Shape of gender in train dataset: {train_dataset_0.gender.shape}\")\n",
    "print(f\"Shape of clusters in train dataset: {train_dataset_0.cluster.shape}\")\n",
    "\n",
    "print(f\"Shape of features in test dataset: {test_dataset_0.features.shape}\")\n",
    "print(f\"Shape of labels in test dataset: {test_dataset_0.labels.shape}\")\n",
    "print(f\"Shape of gender in test dataset: {test_dataset_0.gender.shape}\")\n",
    "print(f\"Shape of clusters in test dataset: {test_dataset_0.cluster.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model with clustering and dropout\n",
    "class ClusterMLP0(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.0):\n",
    "        super(ClusterMLP0, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Two separate output layers for two clusters\n",
    "        self.layer3_cluster1 = nn.Linear(hidden_size2, output_size)\n",
    "        self.layer3_cluster2 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, clusters):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        outputs = []\n",
    "        for x_i, cluster_i in zip(x, clusters):\n",
    "            if cluster_i == 0:\n",
    "                layer = self.layer3_cluster1\n",
    "            elif cluster_i == 1:\n",
    "                layer = self.layer3_cluster2\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected cluster label: {cluster_i}\")\n",
    "\n",
    "            outputs.append(layer(x_i))\n",
    "\n",
    "        return torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for features, labels, gender, cluster, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, cluster)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, gender, cluster, patient_id, sample_weight in val_loader:\n",
    "                outputs = model(features, cluster)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        # Calculate metrics for males and females separately\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "hyperparameter_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "\n",
    "# Cross-validation with hyperparameter tuning\n",
    "best_hyperparams = None\n",
    "best_val_loss = float('inf')\n",
    "best_metrics = None\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for params in ParameterGrid(hyperparameter_grid):\n",
    "    fold_metrics = {\n",
    "        'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "        'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "        'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "    }\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "        print(f'Fold {fold+1} with params: {params}')\n",
    "        train_subset = Subset(train_dataset_0, train_index)\n",
    "        val_subset = Subset(train_dataset_0, val_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        model = ClusterMLP0(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=params['dropout_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "        \n",
    "        fold_metrics['accuracy'].append(accuracy)\n",
    "        fold_metrics['precision'].append(precision)\n",
    "        fold_metrics['sensitivity'].append(sensitivity)\n",
    "        fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "        fold_metrics['male_precision'].append(male_precision)\n",
    "        fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "        fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "        fold_metrics['female_precision'].append(female_precision)\n",
    "        fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "    \n",
    "    avg_val_loss = np.mean([val_loss])\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_hyperparams = params\n",
    "        best_metrics = fold_metrics\n",
    "\n",
    "    # Print the metrics for each combination of hyperparameters at the end of 10 folds\n",
    "    print(f'Hyperparameters: {params}')\n",
    "    for metric in fold_metrics:\n",
    "        print(f'{metric}: Mean = {np.mean(fold_metrics[metric]):.4f}, Std = {np.std(fold_metrics[metric]):.4f}')\n",
    "\n",
    "# Print the best hyperparameters and corresponding metrics\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "for metric in best_metrics:\n",
    "    print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap sampling\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    for pid in np.unique(patient_ids):\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, gender, cluster, patient_id, sample_weight in dataloader:\n",
    "            outputs = model(inputs, cluster)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "\n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    # Gender-specific metrics\n",
    "    males = [i for i, g in enumerate(all_genders) if g == 0]\n",
    "    females = [i for i, g in enumerate(all_genders) if g == 1]\n",
    "\n",
    "    male_labels = [all_labels[i] for i in males]\n",
    "    male_preds = [all_preds[i] for i in males]\n",
    "    female_labels = [all_labels[i] for i in females]\n",
    "    female_preds = [all_preds[i] for i in females]\n",
    "\n",
    "    male_accuracy = accuracy_score(male_labels, male_preds)\n",
    "    male_precision = precision_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "    male_recall = recall_score(male_labels, male_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    male_f1 = f1_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(female_labels, female_preds)\n",
    "    female_precision = precision_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "    female_recall = recall_score(female_labels, female_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    female_f1 = f1_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'overall': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        },\n",
    "        'male': {\n",
    "            'accuracy': male_accuracy,\n",
    "            'precision': male_precision,\n",
    "            'recall': male_recall,\n",
    "            'f1': male_f1\n",
    "        },\n",
    "        'female': {\n",
    "            'accuracy': female_accuracy,\n",
    "            'precision': female_precision,\n",
    "            'recall': female_recall,\n",
    "            'f1': female_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each participant\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset_0, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = ClusterMLP0(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset_0, batch_size=batch_size, shuffle=False), criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset_0, batch_size=batch_size, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results['overall']['accuracy'])\n",
    "    bootstrap_results['precision'].append(results['overall']['precision'])\n",
    "    bootstrap_results['sensitivity'].append(results['overall']['recall'])\n",
    "    bootstrap_results['male_accuracy'].append(results['male']['accuracy'])\n",
    "    bootstrap_results['male_precision'].append(results['male']['precision'])\n",
    "    bootstrap_results['male_sensitivity'].append(results['male']['recall'])\n",
    "    bootstrap_results['female_accuracy'].append(results['female']['accuracy'])\n",
    "    bootstrap_results['female_precision'].append(results['female']['precision'])\n",
    "    bootstrap_results['female_sensitivity'].append(results['female']['recall'])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the KMeans model with the optimal number of clusters on the train data\n",
    "optimal_clusters = 4\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "cluster_train = kmeans.fit_predict(X_train_scaled)\n",
    "\n",
    "# Predict clusters for the test data using the same KMeans model\n",
    "cluster_test = kmeans.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset0(Dataset):\n",
    "    def __init__(self, features, labels, gender, patient_id, cluster, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Constructor for the dataset.\n",
    "        :param features: The input features (numpy array).\n",
    "        :param labels: The labels corresponding to the features (numpy array).\n",
    "        :param gender: The gender information (numpy array).\n",
    "        :param patient_id: The patient ID information (list or numpy array).\n",
    "        :param cluster: The cluster information (numpy array).\n",
    "        :param sample_weight: Optional sample weights (numpy array).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        self.gender = torch.tensor(gender, dtype=torch.long)  # Assuming gender is categorical\n",
    "        self.cluster = torch.tensor(cluster, dtype=torch.long)  # Assuming cluster is categorical\n",
    "        self.patient_id = patient_id\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            print(f\"Sample weight type: {type(sample_weight)}, shape: {np.shape(sample_weight)}\")\n",
    "            self.sample_weight = torch.tensor(sample_weight, dtype=torch.float32)\n",
    "        else:\n",
    "            self.sample_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sample_weight is not None:\n",
    "            return (self.features[index], self.labels[index], \n",
    "                    self.gender[index], self.cluster[index], self.sample_weight[index], \n",
    "                    self.patient_id[index])\n",
    "        return (self.features[index], self.labels[index], \n",
    "                self.gender[index], self.cluster[index], self.patient_id[index])\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset_0 = CustomDataset0(X_train_scaled, y_train, gender_train, ids_train, cluster_train, sample_weight)\n",
    "test_dataset_0 = CustomDataset0(X_test_scaled, y_test, gender_test, ids_test, cluster_test, sample_weight)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 128  # You can adjust this based on your system's capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model with clustering and dropout\n",
    "class ClusterMLP0(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.0):\n",
    "        super(ClusterMLP0, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # four separate output layers for four clusters\n",
    "        self.layer3_cluster1 = nn.Linear(hidden_size2, output_size)\n",
    "        self.layer3_cluster2 = nn.Linear(hidden_size2, output_size)\n",
    "        self.layer3_cluster3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.layer3_cluster4 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, clusters):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        outputs = []\n",
    "        for x_i, cluster_i in zip(x, clusters):\n",
    "            if cluster_i == 0:\n",
    "                layer = self.layer3_cluster1\n",
    "            elif cluster_i == 1:\n",
    "                layer = self.layer3_cluster2\n",
    "            elif cluster_i == 2:\n",
    "                layer = self.layer3_cluster3\n",
    "            elif cluster_i == 3:\n",
    "                layer = self.layer3_cluster4\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected cluster label: {cluster_i}\")\n",
    "\n",
    "            outputs.append(layer(x_i))\n",
    "\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for features, labels, gender, cluster, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, cluster)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, gender, cluster, patient_id, sample_weight in val_loader:\n",
    "                outputs = model(features, cluster)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        # Calculate metrics for males and females separately\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "hyperparameter_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "\n",
    "# Cross-validation with hyperparameter tuning\n",
    "best_hyperparams = None\n",
    "best_val_loss = float('inf')\n",
    "best_metrics = None\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for params in ParameterGrid(hyperparameter_grid):\n",
    "    fold_metrics = {\n",
    "        'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "        'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "        'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "    }\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "        print(f'Fold {fold+1} with params: {params}')\n",
    "        train_subset = Subset(train_dataset_0, train_index)\n",
    "        val_subset = Subset(train_dataset_0, val_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        model = ClusterMLP0(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=params['dropout_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "        \n",
    "        fold_metrics['accuracy'].append(accuracy)\n",
    "        fold_metrics['precision'].append(precision)\n",
    "        fold_metrics['sensitivity'].append(sensitivity)\n",
    "        fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "        fold_metrics['male_precision'].append(male_precision)\n",
    "        fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "        fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "        fold_metrics['female_precision'].append(female_precision)\n",
    "        fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "    \n",
    "    avg_val_loss = np.mean([val_loss])\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_hyperparams = params\n",
    "        best_metrics = fold_metrics\n",
    "\n",
    "    # Print the metrics for each combination of hyperparameters at the end of 10 folds\n",
    "    print(f'Hyperparameters: {params}')\n",
    "    for metric in fold_metrics:\n",
    "        print(f'{metric}: Mean = {np.mean(fold_metrics[metric]):.4f}, Std = {np.std(fold_metrics[metric]):.4f}')\n",
    "\n",
    "# Print the best hyperparameters and corresponding metrics\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "for metric in best_metrics:\n",
    "    print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap sampling\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    for pid in np.unique(patient_ids):\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, gender, cluster, patient_id, sample_weight in dataloader:\n",
    "            outputs = model(inputs, cluster)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "\n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    # Gender-specific metrics\n",
    "    males = [i for i, g in enumerate(all_genders) if g == 0]\n",
    "    females = [i for i, g in enumerate(all_genders) if g == 1]\n",
    "\n",
    "    male_labels = [all_labels[i] for i in males]\n",
    "    male_preds = [all_preds[i] for i in males]\n",
    "    female_labels = [all_labels[i] for i in females]\n",
    "    female_preds = [all_preds[i] for i in females]\n",
    "\n",
    "    male_accuracy = accuracy_score(male_labels, male_preds)\n",
    "    male_precision = precision_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "    male_recall = recall_score(male_labels, male_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    male_f1 = f1_score(male_labels, male_preds, average='binary', zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(female_labels, female_preds)\n",
    "    female_precision = precision_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "    female_recall = recall_score(female_labels, female_preds, average='binary', zero_division=0)  # Sensitivity\n",
    "    female_f1 = f1_score(female_labels, female_preds, average='binary', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'overall': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        },\n",
    "        'male': {\n",
    "            'accuracy': male_accuracy,\n",
    "            'precision': male_precision,\n",
    "            'recall': male_recall,\n",
    "            'f1': male_f1\n",
    "        },\n",
    "        'female': {\n",
    "            'accuracy': female_accuracy,\n",
    "            'precision': female_precision,\n",
    "            'recall': female_recall,\n",
    "            'f1': female_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each participant\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset_0, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = ClusterMLP0(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset_0, batch_size=batch_size, shuffle=False), criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset_0, batch_size=batch_size, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results['overall']['accuracy'])\n",
    "    bootstrap_results['precision'].append(results['overall']['precision'])\n",
    "    bootstrap_results['sensitivity'].append(results['overall']['recall'])\n",
    "    bootstrap_results['male_accuracy'].append(results['male']['accuracy'])\n",
    "    bootstrap_results['male_precision'].append(results['male']['precision'])\n",
    "    bootstrap_results['male_sensitivity'].append(results['male']['recall'])\n",
    "    bootstrap_results['female_accuracy'].append(results['female']['accuracy'])\n",
    "    bootstrap_results['female_precision'].append(results['female']['precision'])\n",
    "    bootstrap_results['female_sensitivity'].append(results['female']['recall'])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uti-minder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
