{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import tqdm\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import sklearn.metrics as skmetrics\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from process_fe import create_feature_engineering_datasets\n",
    "from data import data_to_array_dict, get_data_date_split, get_data_date_id_split, get_feature_colnames\n",
    "from utils import stratification\n",
    "from plotting import paper_theme, ReliabilityDisplay, ShapDisplay, risk_feature_plot\n",
    "import metrics\n",
    "from shap_calculator import calc_shap_df\n",
    "\n",
    "from tqdm_style import tqdm_style\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the training set and predicting on the last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DAYS = 3\n",
    "THRESHOLDS = [0.3, 0.8]\n",
    "DATES_SPLIT = {\n",
    "    \"date_train_start\": \"2021-06-28\",\n",
    "    \"date_train_end\": \"2023-01-01\",\n",
    "    \"date_test_end\": \"2024-01-01\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nice_names = {\n",
    "    'awake_freq': 'Night time Awake Frequency', \n",
    "    'bathroom_daytime_freq': 'Daytime Bathroom Frequency', \n",
    "    'bathroom_daytime_freq_ma': 'Daytime Bathroom Frequency MA', \n",
    "    'bathroom_daytime_freq_ma_delta': 'Daytime Bathroom Frequency MA Delta', \n",
    "    'bathroom_freq': 'Bathroom Frequency',\n",
    "    'bathroom_nighttime_freq': 'Night time Bathroom Frequency', \n",
    "    'bathroom_nighttime_freq_ma': 'Night time Bathroom Frequency MA', \n",
    "    'bathroom_nighttime_freq_ma_delta': 'Night time Bathroom Frequency MA Delta', \n",
    "    'bathroom_relative_transition_time_delta_mean': 'Mean Relative Bathroom Transition Time Delta',\n",
    "    'bathroom_relative_transition_time_delta_std': 'STD Relative Bathroom Transition Time Delta',\n",
    "    'bedroom_freq': 'Bedroom Frequency',\n",
    "    'daily_entropy': 'Daily Entropy', \n",
    "    'hallway_freq': 'Hallway Frequency', \n",
    "    'heart_rate_mean': 'Mean Night Time Heart Rate',\n",
    "    'heart_rate_std': 'STD Night Time Heart Rate', \n",
    "    'kitchen_freq': 'Kitchen Frequency', \n",
    "    'lounge_freq': 'Lounge Frequency', \n",
    "    'previous_uti': 'Number of Previous UTIs',\n",
    "    'respiratory_rate_mean': 'Mean Night Time Respiratory Rate', \n",
    "    'respiratory_rate_std': 'STD Night Time Respiratory Rate',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_data = create_feature_engineering_datasets(reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train, data_test, _ = get_data_date_split(\n",
    "    fe_data, dates_split=DATES_SPLIT, n_days=N_DAYS, impute=True\n",
    ")\n",
    "\n",
    "\n",
    "X_train, y_train, ids_train, sample_weight = (\n",
    "    data_train['X'], data_train['y'], data_train[\"id\"], data_train['sample_weight']\n",
    ")\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to sets\n",
    "set_ids_train = set(ids_train)\n",
    "set_ids_test = set(ids_test)\n",
    "\n",
    "# Check for intersection\n",
    "common_ids = set_ids_train.intersection(set_ids_test)\n",
    "\n",
    "# Check if there are any common elements\n",
    "if common_ids:\n",
    "    print(f\"There are {len(common_ids)} common IDs between train and test datasets.\")\n",
    "else:\n",
    "    print(\"IDs in train and test datasets are unique to each other.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of IDs to remove\n",
    "ids_to_remove = ['AboZyUBeiQW3nVCcbXGpay', 'NZjrVTZQR1w9LPJMt26MbG', 'XVb8nztyc2LYPCAewZq11S', 'XdbAAiDw1vd3Bjbo9EVo1B']\n",
    "\n",
    "# Create a boolean index where False indicates IDs that need to be removed\n",
    "indices_to_keep = ~np.isin(data_test['id'], ids_to_remove)\n",
    "\n",
    "# Use this index to filter all related arrays in data_test\n",
    "data_test['X'] = data_test['X'][indices_to_keep]\n",
    "data_test['y'] = data_test['y'][indices_to_keep]\n",
    "data_test['id'] = data_test['id'][indices_to_keep]\n",
    "data_test['date'] = data_test['date'][indices_to_keep] if 'date' in data_test else None\n",
    "\n",
    "X_test, y_test, ids_test, dates_test = (\n",
    "    data_test['X'], data_test['y'], data_test[\"id\"], data_test['date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a pandas Series\n",
    "ids_train_series = pd.Series(ids_train)\n",
    "\n",
    "# Now you can use the nunique() method\n",
    "unique_count = ids_train_series.nunique()\n",
    "\n",
    "print(\"Number of unique elements in ids_train:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train_series = pd.Series(ids_test)\n",
    "\n",
    "# Now you can use the nunique() method\n",
    "unique_count = ids_train_series.nunique()\n",
    "\n",
    "print(\"Number of unique elements in ids_test:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to sets\n",
    "set_ids_train = set(ids_train)\n",
    "set_ids_test = set(ids_test)\n",
    "\n",
    "# Identify unique IDs in the test set\n",
    "unique_ids_in_test = set_ids_test - set_ids_train\n",
    "\n",
    "print(\"Unique IDs in the test set that are not in the train set:\")\n",
    "print(unique_ids_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_remove1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define flatten function\n",
    "def flatten(x):\n",
    "    return x.reshape(x.shape[0], -1)\n",
    "\n",
    "# Apply flattening\n",
    "X_train_flattened = flatten(X_train)\n",
    "X_test_flattened = flatten(X_test)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_test_scaled = scaler.transform(X_test_flattened)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "sample_weight_torch = torch.tensor(sample_weight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x  # Output raw logits for CrossEntropyLoss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(input_size=X_train_scaled.shape[1], hidden_size1=1000, hidden_size2=10, output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted, labels):\n",
    "    correct = (predicted == labels).float()  # Convert boolean to float for division\n",
    "    acc = correct.sum() / len(labels)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_mlp = torch.optim.Adam(\n",
    "    params=mlp_model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9,0.999)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store loss, accuracy, and other metrics for each epoch\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "epoch_precisions = []\n",
    "epoch_recalls = []\n",
    "epoch_f1s = []\n",
    "epoch_avg_precisions = []\n",
    "\n",
    "# this is for the KL divergence matrix\n",
    "losses_per_patient = {}\n",
    "patient_ids = np.unique(ids_train)  # Assuming ids_train is a NumPy array\n",
    "# Initialize the dictionary for each patient ID\n",
    "for patient_id in patient_ids:\n",
    "    losses_per_patient[patient_id] = []\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    mlp_model.train()\n",
    "    optimizer_mlp.zero_grad()\n",
    "    outputs = mlp_model(X_train_torch)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    correct = (predicted.squeeze() == y_train_torch).float().sum()\n",
    "    train_accuracy = correct / len(y_train_torch)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, y_train_torch.long())\n",
    "    loss.backward()\n",
    "    optimizer_mlp.step()\n",
    "\n",
    "    if epoch == 2:  # Third epoch (zero-indexed)\n",
    "        model_state = mlp_model.state_dict()\n",
    "        for i, (input, label) in enumerate(zip(X_train_torch, y_train_torch)):\n",
    "            output = mlp_model(input.unsqueeze(0))  # Process each sample individually\n",
    "            loss = criterion(output, label.unsqueeze(0).long())\n",
    "            losses_per_patient[ids_train[i]].append(loss.item())\n",
    "    \n",
    "\n",
    "    # Store metrics for this epoch\n",
    "    epoch_losses.append(loss.item())\n",
    "    epoch_accuracies.append(train_accuracy.item() * 100)\n",
    "\n",
    "    precision = precision_score(y_train_torch.cpu().numpy(), predicted.cpu().numpy())\n",
    "    recall = recall_score(y_train_torch.cpu().numpy(), predicted.cpu().numpy())\n",
    "    f1 = f1_score(y_train_torch.cpu().numpy(), predicted.cpu().numpy())\n",
    "    \n",
    "    # Assuming outputs are logits from your mlp_model with shape [batch_size, 2]\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    probabilities = softmax(outputs)  # Convert logits to probabilities\n",
    "    positive_class_probs = probabilities[:, 1]  # Extract probabilities for the positive class\n",
    "\n",
    "    # Convert tensors to CPU numpy arrays for use with scikit-learn\n",
    "    y_true_numpy = y_train_torch.cpu().numpy()\n",
    "    positive_class_probs_numpy = positive_class_probs.cpu().detach().numpy()\n",
    "\n",
    "    # Calculate average precision score\n",
    "    average_precision = average_precision_score(y_true_numpy, positive_class_probs_numpy)\n",
    "\n",
    "    epoch_precisions.append(precision)\n",
    "    epoch_recalls.append(recall)\n",
    "    epoch_f1s.append(f1)\n",
    "    epoch_avg_precisions.append(average_precision)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {train_accuracy.item() * 100:.2f}%')\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Average Precision: {average_precision:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_losses, label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_accuracies, label='Training Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_losses, label='Training Loss', color='deepskyblue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Set x-axis interval to 2\n",
    "plt.xticks(range(0, len(epoch_losses), 2))\n",
    "\n",
    "# Move the legend above the plot\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1))\n",
    "\n",
    "# Add a dashed grid line\n",
    "plt.grid(True, linestyle='--', color='grey')\n",
    "\n",
    "# Remove the top and right spines\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_at_epoch(mlp_model, X_test_torch, y_test_torch, ids_test, criterion, target_epoch_model_state):\n",
    "    mlp_model.load_state_dict(target_epoch_model_state)  # Load the saved model state\n",
    "    mlp_model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Initialize an empty dictionary for test participant losses\n",
    "    losses_per_patient_test = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = mlp_model(X_test_torch)\n",
    "        for i in range(len(y_test_torch)):\n",
    "            patient_id = ids_test[i]  # Assuming ids_test contains string elements\n",
    "            if patient_id not in losses_per_patient_test:\n",
    "                losses_per_patient_test[patient_id] = []\n",
    "            individual_loss = criterion(outputs[i].unsqueeze(0), y_test_torch[i].unsqueeze(0).long()).item()\n",
    "            losses_per_patient_test[patient_id].append(individual_loss)\n",
    "\n",
    "    # Calculate average loss per test participant\n",
    "    target_epoch_test_participant_losses = []\n",
    "    for patient_id in losses_per_patient_test:\n",
    "        if losses_per_patient_test[patient_id]:  # Avoid division by zero\n",
    "            avg_loss = sum(losses_per_patient_test[patient_id]) / len(losses_per_patient_test[patient_id])\n",
    "            target_epoch_test_participant_losses.append((patient_id, avg_loss))\n",
    "\n",
    "    return target_epoch_test_participant_losses\n",
    "\n",
    "# Example call to the evaluation function\n",
    "target_epoch_test_participant_losses = evaluate_model_at_epoch(\n",
    "    mlp_model, X_test_torch, y_test_torch, ids_test, criterion, model_state\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Test participants' average losses at target epoch:\")\n",
    "for participant_id, avg_loss in target_epoch_test_participant_losses:\n",
    "    print(f\"Participant {participant_id}: Average Loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map['gender_encoded'] = gender_map['Gender PwD'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "\n",
    "patient_gender_dict = dict(zip(gender_map['patient_id'], gender_map['gender_encoded']))\n",
    "\n",
    "gender_train = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_train] \n",
    "\n",
    "gender_test = [patient_gender_dict.get(patient_id, 0) for patient_id in ids_test] \n",
    "\n",
    "gender_train = np.array(gender_train).astype(int)\n",
    "gender_test = np.array(gender_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "average_losses = []\n",
    "for patient_id, losses in losses_per_patient.items():\n",
    "    if losses:  # Check if there are any losses recorded to avoid division by zero\n",
    "        average_loss = sum(losses) / len(losses)\n",
    "        average_losses.append(average_loss)\n",
    "    else:\n",
    "        average_losses.append(0)  # Append 0 or an appropriate value if no losses were recorded\n",
    "\n",
    "\n",
    "\n",
    "# Assuming average_losses is a list containing all the average losses\n",
    "mean_average_losses = statistics.mean(average_losses)\n",
    "std_dev_average_losses = statistics.stdev(average_losses)\n",
    "mean_plus1_sd = mean_average_losses + std_dev_average_losses\n",
    "\n",
    "print(\"Mean of Average Losses:\", mean_average_losses)\n",
    "print(\"Standard Deviation of Average Losses:\", std_dev_average_losses)\n",
    "print(\"Mean + 1 SD:\", mean_plus1_sd)\n",
    "\n",
    "\n",
    "# Plotting the histogram of average losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(average_losses, bins=15, color='steelblue', alpha=0.75)  # You can adjust the number of bins\n",
    "plt.title('Histogram of Average Losses at Epoch 3 for All Patients')\n",
    "plt.xlabel('Average Loss')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.grid(True)\n",
    "\n",
    "# Adding vertical lines for mean and mean + 1 SD\n",
    "plt.axvline(mean_average_losses, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(mean_average_losses + std_dev_average_losses, color='blue', linestyle='dashed', linewidth=2)\n",
    "\n",
    "# Adding a legend to identify the lines\n",
    "plt.legend(['Mean', 'Mean + 1 SD'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming average_losses is a list containing all the average losses\n",
    "average_losses_array = np.array(average_losses).reshape(-1, 1)\n",
    "\n",
    "# Using the elbow method to find the optimal number of clusters\n",
    "sum_of_squared_distances = []\n",
    "K = range(1, 10)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans = kmeans.fit(average_losses_array)\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow method graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming average_losses_array is defined and populated\n",
    "# Fit the KMeans model with the optimal number of clusters (assuming it is 3)\n",
    "optimal_k = 3  # Replace with the actual optimal k determined from the elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_k)\n",
    "kmeans.fit(average_losses_array)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the average_losses_array for visualization\n",
    "clustered_losses = np.concatenate((average_losses_array, labels.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# Plotting the histogram with cluster labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['lightseagreen', 'cornflowerblue', 'slateblue']\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = clustered_losses[clustered_losses[:, 1] == cluster][:, 0]\n",
    "    plt.hist(cluster_data, bins=15, alpha=0.75, label=f'Cluster {cluster + 1}', color=colors[cluster])\n",
    "\n",
    "plt.xlabel('Average Loss')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Adding a legend to identify the clusters\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.10), ncol=3)\n",
    "\n",
    "# Remove the top and right spines\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many patients are in cluster 2\n",
    "cluster_2_count = np.sum(labels == 2)\n",
    "print(f\"Number of patients in cluster 2: {cluster_2_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_losses_array = np.array([loss for _, loss in target_epoch_test_participant_losses]).reshape(-1, 1)\n",
    "\n",
    "# Use the pre-trained KMeans model to predict the cluster labels for the new data\n",
    "new_labels = kmeans.predict(new_losses_array)\n",
    "\n",
    "# Combine participant IDs with their corresponding new cluster labels for clarity\n",
    "new_clustered_results = [(participant_id, avg_loss, cluster_label) \n",
    "                         for (participant_id, avg_loss), cluster_label in zip(target_epoch_test_participant_losses, new_labels)]\n",
    "\n",
    "# Print the clustered results\n",
    "print(\"New clustered results (Participant ID, Average Loss, Cluster Label):\")\n",
    "for participant_id, avg_loss, cluster_label in new_clustered_results:\n",
    "    print(f\"Participant {participant_id}: Average Loss = {avg_loss:.4f}, Cluster = {cluster_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gender distribution in training data:\", np.unique(gender_train, return_counts=True))\n",
    "print(\"Gender distribution in testing data:\", np.unique(gender_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Step 1: Convert ids_train and ids_test to sets\n",
    "set_ids_train = set(ids_train)\n",
    "set_ids_test = set(ids_test)\n",
    "\n",
    "# Ensure all ids in test can be found in train\n",
    "common_ids = set_ids_test.intersection(set_ids_train)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of common IDs between train and test dataset: {len(common_ids)}\")\n",
    "if common_ids:\n",
    "    print(f\"Common IDs: {common_ids}\")\n",
    "\n",
    "# Identify unique IDs in the test set\n",
    "unique_ids_in_test = set_ids_test - set_ids_train\n",
    "\n",
    "print(\"Unique IDs in the test set that are not in the train set:\")\n",
    "print(unique_ids_in_test)\n",
    "\n",
    "# Create a dictionary for the new clustered results\n",
    "new_clustered_dict = {participant_id: cluster_label for participant_id, avg_loss, cluster_label in new_clustered_results}\n",
    "\n",
    "# Generate loss_label_test based on the ID type\n",
    "loss_label_test = []\n",
    "for patient_id in ids_test:\n",
    "    if patient_id in common_ids:\n",
    "        loss_label_test.append(loss_labels.get(patient_id, -1))\n",
    "    elif patient_id in unique_ids_in_test:\n",
    "        loss_label_test.append(new_clustered_dict.get(patient_id, -1))\n",
    "    else:\n",
    "        loss_label_test.append(-1)\n",
    "        warnings.warn(f\"Patient ID {patient_id} not found in either training set or new clustered results.\")\n",
    "\n",
    "# Print the first few labels for verification\n",
    "print(\"First few loss labels for test set:\", loss_label_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare dictionaries to hold counts for each cluster\n",
    "common_ids_count = Counter()\n",
    "unique_ids_in_test_count = Counter()\n",
    "unique_ids_in_train_count = Counter()\n",
    "\n",
    "# Count common IDs\n",
    "for patient_id in common_ids:\n",
    "    cluster_label = loss_labels.get(patient_id, -1)\n",
    "    if cluster_label != -1:\n",
    "        common_ids_count[cluster_label] += 1\n",
    "\n",
    "# Count unique IDs in test set\n",
    "for patient_id in unique_ids_in_test:\n",
    "    cluster_label = new_clustered_dict.get(patient_id, -1)\n",
    "    if cluster_label != -1:\n",
    "        unique_ids_in_test_count[cluster_label] += 1\n",
    "\n",
    "# Identify unique IDs in the train set\n",
    "unique_ids_in_train = set_ids_train - set_ids_test\n",
    "\n",
    "# Count unique IDs in train set\n",
    "for patient_id in unique_ids_in_train:\n",
    "    cluster_label = loss_labels.get(patient_id, -1)\n",
    "    if cluster_label != -1:\n",
    "        unique_ids_in_train_count[cluster_label] += 1\n",
    "\n",
    "# Step 2: Prepare data for plotting\n",
    "id_groups = ['Unique IDs in Train', 'Common IDs', 'Unique IDs in Test']\n",
    "common_counts = [common_ids_count[cluster] for cluster in range(3)]\n",
    "unique_test_counts = [unique_ids_in_test_count[cluster] for cluster in range(3)]\n",
    "unique_train_counts = [unique_ids_in_train_count[cluster] for cluster in range(3)]\n",
    "\n",
    "counts = [unique_train_counts, common_counts, unique_test_counts]\n",
    "\n",
    "colors = ['lightseagreen', 'cornflowerblue', 'slateblue']\n",
    "\n",
    "# Step 3: Plotting\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(id_groups))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for cluster in range(3):\n",
    "    counts_for_cluster = [unique_train_counts[cluster], common_counts[cluster], unique_test_counts[cluster]]\n",
    "    ax.bar(index + cluster * bar_width, counts_for_cluster, bar_width, label=f'Cluster {cluster + 1}', color=colors[cluster])\n",
    "\n",
    "ax.set_xlabel('ID Groupings')\n",
    "ax.set_ylabel('Number of Patients')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(id_groups)\n",
    "ax.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=3)\n",
    "\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert ids_train and ids_test to sets\n",
    "set_ids_train = set(ids_train)\n",
    "set_ids_test = set(ids_test)\n",
    "\n",
    "# Ensure all ids in test can be found in train\n",
    "common_ids = set_ids_test.intersection(set_ids_train)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of common IDs between train and test dataset: {len(common_ids)}\")\n",
    "if common_ids:\n",
    "    print(f\"Common IDs: {common_ids}\")\n",
    "\n",
    "# Generate loss_label_test\n",
    "loss_label_test = [loss_labels.get(patient_id, -1) for patient_id in ids_test]\n",
    "\n",
    "# Print the first few labels for verification\n",
    "print(\"First few loss labels for test set:\", loss_label_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss distribution in training data:\", np.unique(loss_label_train, return_counts=True))\n",
    "print(\"Loss distribution in test data:\", np.unique(loss_label_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels,loss_labels, gender, patient_id, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Constructor for the dataset.\n",
    "        :param features: The input features (numpy array).\n",
    "        :param labels: The labels corresponding to the features (numpy array).\n",
    "        :param gender: The gender information (numpy array).\n",
    "        :param sample_weight: Optional sample weights (numpy array).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.loss_labels = torch.tensor(loss_labels, dtype=torch.long)\n",
    "        self.gender = torch.tensor(gender, dtype=torch.long)  # Assuming gender is categorical\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            self.sample_weight = torch.tensor(sample_weight, dtype=torch.float32)\n",
    "        else:\n",
    "            self.sample_weight = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sample_weight is not None:\n",
    "            return self.features[index], self.labels[index],self.loss_labels[index], self.gender[index], self.sample_weight[index], self.patient_id[index]\n",
    "        return self.features[index], self.labels[index], self.loss_labels[index], self.patient_id[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = CustomDataset(X_train_scaled, y_train, loss_label_train, gender_train, ids_train, sample_weight)\n",
    "test_dataset = CustomDataset(X_test_scaled, y_test, loss_label_test, gender_test, ids_test, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 128  # You can adjust this based on your system's capabilities\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def check_loss_label_distribution(dataset):\n",
    "    loss_label_counts = Counter()\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    for _, _, loss_label, _, _, _ in dataset:\n",
    "        # Update count of each loss label\n",
    "        loss_label_counts[loss_label.item()] += 1\n",
    "\n",
    "    return loss_label_counts\n",
    "\n",
    "# Check the loss label distribution in the training dataset\n",
    "train_loss_label_distribution = check_loss_label_distribution(train_dataset)\n",
    "print(\"Loss label distribution in training dataset:\", train_loss_label_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final layer separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class with dropout\n",
    "class LossLabelMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate):\n",
    "        super(LossLabelMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.layer3_cluster0 = nn.Linear(hidden_size2, output_size)\n",
    "        self.layer3_cluster1 = nn.Linear(hidden_size2, output_size)\n",
    "        self.layer3_cluster2 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, loss_labels):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        outputs = []\n",
    "        for x_i, loss_label_i in zip(x, loss_labels):\n",
    "            if loss_label_i == 0:\n",
    "                layer = self.layer3_cluster0\n",
    "            elif loss_label_i == 1:\n",
    "                layer = self.layer3_cluster1\n",
    "            elif loss_label_i == 2:\n",
    "                layer = self.layer3_cluster2\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected loss_label: {loss_label_i}\")\n",
    "            outputs.append(layer(x_i))\n",
    "\n",
    "        return torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for features, labels, loss_labels, gender, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, loss_labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, loss_labels, gender, patient_id, sample_weight in val_loader:\n",
    "                outputs = model(features, loss_labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_and_tune(train_dataset, num_unique_ids, param_grid, num_folds=10, num_epochs=10):\n",
    "    best_hyperparams = None\n",
    "    best_val_loss = float('inf')\n",
    "    best_metrics = None\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_folds)\n",
    "    \n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"Testing hyperparameters: {params}\")\n",
    "        \n",
    "        fold_metrics = {\n",
    "            'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "            'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "            'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "        }\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "            print(f'Fold {fold+1}/{num_folds} with params: {params}')\n",
    "            \n",
    "            train_subset = Subset(train_dataset, train_index)\n",
    "            val_subset = Subset(train_dataset, val_index)\n",
    "            train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "            \n",
    "            model = LossLabelMLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=params['dropout_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            \n",
    "            val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "            \n",
    "            fold_metrics['accuracy'].append(accuracy)\n",
    "            fold_metrics['precision'].append(precision)\n",
    "            fold_metrics['sensitivity'].append(sensitivity)\n",
    "            fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "            fold_metrics['male_precision'].append(male_precision)\n",
    "            fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "            fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "            fold_metrics['female_precision'].append(female_precision)\n",
    "            fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "        \n",
    "        avg_val_loss = np.mean([val_loss])\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_hyperparams = params\n",
    "            best_metrics = fold_metrics\n",
    "\n",
    "        # Print the metrics for each combination of hyperparameters at the end of 10 folds\n",
    "        print(f'Hyperparameters: {params}')\n",
    "        for metric in fold_metrics:\n",
    "            print(f'{metric}: Mean = {np.mean(fold_metrics[metric]):.4f}, Std = {np.std(fold_metrics[metric]):.4f}')\n",
    "\n",
    "    # Print the best hyperparameters and corresponding metrics\n",
    "    print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "    for metric in best_metrics:\n",
    "        print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')\n",
    "\n",
    "    return best_hyperparams\n",
    "\n",
    "# Perform cross-validation and find the best hyperparameters\n",
    "param_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "num_unique_ids = len(ids_train)\n",
    "\n",
    "best_hyperparams = cross_validate_and_tune(train_dataset, num_unique_ids, param_grid, num_folds=10, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap sampling\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    for pid in np.unique(patient_ids):\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels, loss_labels, gender, patient_id, sample_weight in dataloader:\n",
    "            outputs = model(features, loss_labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    male_indices = np.array(all_genders) == 0\n",
    "    female_indices = np.array(all_genders) == 1\n",
    "\n",
    "    male_accuracy = accuracy_score(np.array(all_labels)[male_indices], np.array(all_preds)[male_indices])\n",
    "    male_precision = precision_score(np.array(all_labels)[male_indices], np.array(all_preds)[male_indices], average='binary', zero_division=0)\n",
    "    male_sensitivity = recall_score(np.array(all_labels)[male_indices], np.array(all_preds)[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(np.array(all_labels)[female_indices], np.array(all_preds)[female_indices])\n",
    "    female_precision = precision_score(np.array(all_labels)[female_indices], np.array(all_preds)[female_indices], average='binary', zero_division=0)\n",
    "    female_sensitivity = recall_score(np.array(all_labels)[female_indices], np.array(all_preds)[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'sensitivity': sensitivity,\n",
    "        'male_accuracy': male_accuracy,\n",
    "        'male_precision': male_precision,\n",
    "        'male_sensitivity': male_sensitivity,\n",
    "        'female_accuracy': female_accuracy,\n",
    "        'female_precision': female_precision,\n",
    "        'female_sensitivity': female_sensitivity\n",
    "    }\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each patient\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    model = LossLabelMLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset, batch_size=128, shuffle=False), criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader, criterion)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results['accuracy'])\n",
    "    bootstrap_results['precision'].append(results['precision'])\n",
    "    bootstrap_results['sensitivity'].append(results['sensitivity'])\n",
    "    bootstrap_results['male_accuracy'].append(results['male_accuracy'])\n",
    "    bootstrap_results['male_precision'].append(results['male_precision'])\n",
    "    bootstrap_results['male_sensitivity'].append(results['male_sensitivity'])\n",
    "    bootstrap_results['female_accuracy'].append(results['female_accuracy'])\n",
    "    bootstrap_results['female_precision'].append(results['female_precision'])\n",
    "    bootstrap_results['female_sensitivity'].append(results['female_sensitivity'])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fully separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLabelMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate):\n",
    "        super(LossLabelMLP, self).__init__()\n",
    "        \n",
    "        # Define layers for cluster 0\n",
    "        self.layer1_cluster0 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1_cluster0 = nn.Dropout(dropout_rate)\n",
    "        self.layer2_cluster0 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2_cluster0 = nn.Dropout(dropout_rate)\n",
    "        self.layer3_cluster0 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        # Define layers for cluster 1\n",
    "        self.layer1_cluster1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1_cluster1 = nn.Dropout(dropout_rate)\n",
    "        self.layer2_cluster1 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2_cluster1 = nn.Dropout(dropout_rate)\n",
    "        self.layer3_cluster1 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        # Define layers for cluster 2\n",
    "        self.layer1_cluster2 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1_cluster2 = nn.Dropout(dropout_rate)\n",
    "        self.layer2_cluster2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2_cluster2 = nn.Dropout(dropout_rate)\n",
    "        self.layer3_cluster2 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, loss_labels):\n",
    "        outputs = []\n",
    "        for x_i, loss_label_i in zip(x, loss_labels):\n",
    "            if loss_label_i == 0:\n",
    "                x_i = self.layer1_cluster0(x_i)\n",
    "                x_i = self.relu(x_i)\n",
    "                x_i = self.dropout1_cluster0(x_i)\n",
    "                x_i = self.layer2_cluster0(x_i)\n",
    "                x_i = self.relu(x_i)\n",
    "                x_i = self.dropout2_cluster0(x_i)\n",
    "                x_i = self.layer3_cluster0(x_i)\n",
    "            elif loss_label_i == 1:\n",
    "                x_i = self.layer1_cluster1(x_i)\n",
    "                x_i = self.relu(x_i)\n",
    "                x_i = self.dropout1_cluster1(x_i)\n",
    "                x_i = self.layer2_cluster1(x_i)\n",
    "                x_i = self.relu(x_i)\n",
    "                x_i = self.dropout2_cluster1(x_i)\n",
    "                x_i = self.layer3_cluster1(x_i)\n",
    "            elif loss_label_i == 2:\n",
    "                x_i = self.layer1_cluster2(x_i)\n",
    "                x_i = self.relu(x_i)\n",
    "                x_i = self.dropout1_cluster2(x_i)\n",
    "                x_i = self.layer2_cluster2(x_i)\n",
    "                x_i = self.relu(x_i)\n",
    "                x_i = self.dropout2_cluster2(x_i)\n",
    "                x_i = self.layer3_cluster2(x_i)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected loss_label: {loss_label_i}\")\n",
    "            outputs.append(x_i)\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for features, labels, loss_labels, gender, patient_id, sample_weight in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, loss_labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_genders = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels, loss_labels, gender, patient_id, sample_weight in val_loader:\n",
    "                outputs = model(features, loss_labels)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_genders.extend(gender.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_genders = np.array(all_genders)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "        sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "        male_indices = all_genders == 0\n",
    "        female_indices = all_genders == 1\n",
    "\n",
    "        male_accuracy = accuracy_score(all_labels[male_indices], all_preds[male_indices])\n",
    "        male_precision = precision_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "        male_sensitivity = recall_score(all_labels[male_indices], all_preds[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "        female_accuracy = accuracy_score(all_labels[female_indices], all_preds[female_indices])\n",
    "        female_precision = precision_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "        female_sensitivity = recall_score(all_labels[female_indices], all_preds[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Overall - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}, '\n",
    "              f'Male - Accuracy: {male_accuracy:.4f}, Precision: {male_precision:.4f}, Sensitivity: {male_sensitivity:.4f}, '\n",
    "              f'Female - Accuracy: {female_accuracy:.4f}, Precision: {female_precision:.4f}, Sensitivity: {female_sensitivity:.4f}')\n",
    "    \n",
    "    return val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_and_tune(train_dataset, num_unique_ids, param_grid, num_folds=10, num_epochs=10):\n",
    "    best_hyperparams = None\n",
    "    best_val_loss = float('inf')\n",
    "    best_metrics = None\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_folds)\n",
    "    \n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"Testing hyperparameters: {params}\")\n",
    "        \n",
    "        fold_metrics = {\n",
    "            'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "            'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "            'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "        }\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(skf.split(X_train_scaled, y_train)):\n",
    "            print(f'Fold {fold+1}/{num_folds} with params: {params}')\n",
    "            \n",
    "            train_subset = Subset(train_dataset, train_index)\n",
    "            val_subset = Subset(train_dataset, val_index)\n",
    "            train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "            \n",
    "            model = LossLabelMLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=params['dropout_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "            \n",
    "            val_loss, accuracy, precision, sensitivity, male_accuracy, male_precision, male_sensitivity, female_accuracy, female_precision, female_sensitivity = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "            \n",
    "            fold_metrics['accuracy'].append(accuracy)\n",
    "            fold_metrics['precision'].append(precision)\n",
    "            fold_metrics['sensitivity'].append(sensitivity)\n",
    "            fold_metrics['male_accuracy'].append(male_accuracy)\n",
    "            fold_metrics['male_precision'].append(male_precision)\n",
    "            fold_metrics['male_sensitivity'].append(male_sensitivity)\n",
    "            fold_metrics['female_accuracy'].append(female_accuracy)\n",
    "            fold_metrics['female_precision'].append(female_precision)\n",
    "            fold_metrics['female_sensitivity'].append(female_sensitivity)\n",
    "        \n",
    "        avg_val_loss = np.mean([val_loss])\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_hyperparams = params\n",
    "            best_metrics = fold_metrics\n",
    "\n",
    "        # Print the metrics for each combination of hyperparameters at the end of 10 folds\n",
    "        print(f'Hyperparameters: {params}')\n",
    "        for metric in fold_metrics:\n",
    "            print(f'{metric}: Mean = {np.mean(fold_metrics[metric]):.4f}, Std = {np.std(fold_metrics[metric]):.4f}')\n",
    "\n",
    "    # Print the best hyperparameters and corresponding metrics\n",
    "    print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "    for metric in best_metrics:\n",
    "        print(f'{metric}: Mean = {np.mean(best_metrics[metric]):.4f}, Std = {np.std(best_metrics[metric]):.4f}')\n",
    "\n",
    "    return best_hyperparams\n",
    "\n",
    "# Perform cross-validation and find the best hyperparameters\n",
    "param_grid = {'lr': [0.001, 0.005, 0.01], 'dropout_rate': [0, 0.2, 0.5]}\n",
    "num_unique_ids = len(ids_train)\n",
    "\n",
    "best_hyperparams = cross_validate_and_tune(train_dataset, num_unique_ids, param_grid, num_folds=10, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrap sampling\n",
    "def bootstrap_sample(dataset, patient_ids, proportion=0.8):\n",
    "    sampled_indices = []\n",
    "    for pid in np.unique(patient_ids):\n",
    "        pid_indices = np.where(patient_ids == pid)[0]\n",
    "        sample_size = int(proportion * len(pid_indices))\n",
    "        sampled_pid_indices = np.random.choice(pid_indices, size=sample_size, replace=True)\n",
    "        sampled_indices.extend(sampled_pid_indices)\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_genders = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels, loss_labels, gender, patient_id, sample_weight in dataloader:\n",
    "            outputs = model(features, loss_labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_genders.extend(gender.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    sensitivity = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "\n",
    "    male_indices = np.array(all_genders) == 0\n",
    "    female_indices = np.array(all_genders) == 1\n",
    "\n",
    "    male_accuracy = accuracy_score(np.array(all_labels)[male_indices], np.array(all_preds)[male_indices])\n",
    "    male_precision = precision_score(np.array(all_labels)[male_indices], np.array(all_preds)[male_indices], average='binary', zero_division=0)\n",
    "    male_sensitivity = recall_score(np.array(all_labels)[male_indices], np.array(all_preds)[male_indices], average='binary', zero_division=0)\n",
    "\n",
    "    female_accuracy = accuracy_score(np.array(all_labels)[female_indices], np.array(all_preds)[female_indices])\n",
    "    female_precision = precision_score(np.array(all_labels)[female_indices], np.array(all_preds)[female_indices], average='binary', zero_division=0)\n",
    "    female_sensitivity = recall_score(np.array(all_labels)[female_indices], np.array(all_preds)[female_indices], average='binary', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'sensitivity': sensitivity,\n",
    "        'male_accuracy': male_accuracy,\n",
    "        'male_precision': male_precision,\n",
    "        'male_sensitivity': male_sensitivity,\n",
    "        'female_accuracy': female_accuracy,\n",
    "        'female_precision': female_precision,\n",
    "        'female_sensitivity': female_sensitivity\n",
    "    }\n",
    "\n",
    "# Bootstrap sampling and training the best model\n",
    "num_bootstrap_samples = 5\n",
    "bootstrap_results = {\n",
    "    'accuracy': [], 'precision': [], 'sensitivity': [],\n",
    "    'male_accuracy': [], 'male_precision': [], 'male_sensitivity': [],\n",
    "    'female_accuracy': [], 'female_precision': [], 'female_sensitivity': []\n",
    "}\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Bootstrap sampling 80% of data points for each patient\n",
    "    bootstrap_subset = bootstrap_sample(train_dataset, ids_train, proportion=0.8)\n",
    "    bootstrap_loader = DataLoader(bootstrap_subset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    model = LossLabelMLP(input_size=X_train_scaled.shape[1], hidden_size1=30, hidden_size2=10, output_size=2, dropout_rate=best_hyperparams['dropout_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    \n",
    "    # Train with the best hyperparameters\n",
    "    train_model(model, bootstrap_loader, DataLoader(test_dataset, batch_size=128, shuffle=False), criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on the full test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    results = evaluate_model(model, test_loader, criterion)\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(results['accuracy'])\n",
    "    bootstrap_results['precision'].append(results['precision'])\n",
    "    bootstrap_results['sensitivity'].append(results['sensitivity'])\n",
    "    bootstrap_results['male_accuracy'].append(results['male_accuracy'])\n",
    "    bootstrap_results['male_precision'].append(results['male_precision'])\n",
    "    bootstrap_results['male_sensitivity'].append(results['male_sensitivity'])\n",
    "    bootstrap_results['female_accuracy'].append(results['female_accuracy'])\n",
    "    bootstrap_results['female_precision'].append(results['female_precision'])\n",
    "    bootstrap_results['female_sensitivity'].append(results['female_sensitivity'])\n",
    "\n",
    "# Print bootstrap results\n",
    "for metric in bootstrap_results:\n",
    "    print(f'{metric.capitalize()}: Mean = {np.mean(bootstrap_results[metric]):.4f}, Std = {np.std(bootstrap_results[metric]):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uti-minder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
